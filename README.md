# Perceptron Cache Attack Detection
This repository contains code for reproducing some of the results of the [PerSpectron paper](https://ieeexplore.ieee.org/document/9251956).

Specifically, this repository's directories contain the following items:
* `custom_benchmarks/`: A few programs that perform cache exploits like Prime+Probe, Flush+Reload, and Flush+Flush, along with some simple benign programs for comparison.
* `dataset_gen/`: Python scripts that help simulate programs in gem5 while periodically dumping performance counters, along with scripts that combine dumps into a single csv dataset.
* `datasets/`: Some premade datasets with the configurations used to generate them.
* `extra_SPEC2006_samples.tar.gz`: Raw performance counter samples generated by simulating SPEC2006 programs.
* `main_samples.tar.gz`: Raw performance counter samples generated by simulating the programs in `custom_benchmarks/`.
* `raw_samples/`: Empty directory used during the dataset creation process (see `raw_samples/README.txt`).
* `training/`: Python scripts for splitting datasets into training/testing sets, training a perceptron model, and evaluating the performance of models.

For instructions on how to generate raw samples from benchmarks and how to generate csv datasets from samples, see the "Generating the Dataset" section.

Otherwise, see the "Testing Models" section for instructions on how to use datasets for testing.

## Generating the Dataset
Creating a dataset is a three-step process: generating raw samples, computing the bounds of the non-constant datapoints, and forming the final csv dataset.

### Generating raw samples
First, download and install the gem5 simulator (version 21.2.1.0 has been tested; later versions should work if nothing gets deprecated). I recommend placing the gem5 directory at the root of this repository (i.e., `perceptron-cache-attack-detection/gem5/`).

If you want to generate samples using SPEC 2006 programs:
* Install SPEC 2006 and run `source shrc` in the SPEC2006 directory to set up the environment variables. Again, I recommend installing the files to `perceptron-cache-attack-detection/SPEC2006/`. 
* Modify `SPEC_PATH` inside `dataset_gen/config.py` (again, this should be already correct if you placed the `SPEC2006` directory in the suggested spot).
* Execute the following command inside the SPEC2006 directory to build some of the programs: `runspec --config=Example-linux64-amd64-gcc41.cfg --action=build --tune=base bzip2 gcc mcf gobmk hmmer sjeng libquantum h264ref omnetpp astar`. (These are 10 of the integer-based programs that I was able to build; feel free to add more.)
* Inside your SPEC2006 directory, check inside `benchspec/CPU2006/401.bzip2/run` and check the name of the folder that was generated. Modify `RUN_DIR_suffix` in `dataset_gen/config.py` to match the name (for example, `/run/run_base_ref_amd64-m64-gcc41-nn.0000/`).
* Finally, inside the bzip run folder, locate the executable (should be similar to `bzip2_base.amd64-m64-gcc41-nn`). Modify `x86_suffix` inside `dataset_gen/config.py` to match the suffix of the executable (for example, `_base.amd64-m64-gcc41-nn`, i.e. the part after `bzip`).

If you have not yet already done so, you should open `dataset_gen/config.py` and look over the settings. Particularly:
* Ensure that `SPEC_PATH`, `CUSTOM_PATH`, and `OUTPUT_DIR` are all valid paths.
* If any of the benchmarks that you plan to run contain an initial start-up period that you consider to be unrepresentative of the benchmark, then ensure that `SKIPPED_TIME` is set to a sufficiently high value.
* Ensure that `SAMPLING_PERIODS` contains the period(s) at which you want to generate samples. Note that only samples with the largest period listed will be contiguous in simulated time; samples with shorter periods will be formed by trimming the longest sample. So, if you need samples to be contiguous (like if you want to try out the sample concat feature during model training/testing), then it is sufficient to set this to a single value. (See the description of `SAMPLING_PERIODS` in `config.py` for more info.)
* Ensure that `BATCHES_PER_BENCHMARK` and `SAMPLES_PER_BATCH` are sufficiently high. (If you find yourself changing these values a lot, consider modifying `dataset_gen/run_simulation.py` to take an extra command line argument that overwrites one of these values.)
* Look over the list of flags that get parsed (these are the same flags used by `gem5/configs/example/se.py`).
* Look over the invocations of `SPECBenchmarkBuilder` and `CustomBenchmarkBuilder` to see how the SPEC2006 and custom programs are called. (For more info on the custom benchmarks, see the "Custom Benchmarks" section.)
* If you have additional binaries that you want to generate samples for, you can place them (plus any required input files) inside the `CUSTOM_PATH` folder (`perceptron-cache-attack-detection/custom_benchmarks` by default). You can then use a CustomBenchmarkBuilder call to register the executable and tie it to a benchmark name (which gets passed as a command line argument for `run_simulation.py`).

In order to execute `run_simulation.py`, you will need to copy both `run_simulation.py` and `sim_config.py` to the `gem5/configs` directory in your gem5 installation. Then, assuming your working directory is at the root of the `gem5` repo, you can execute `run_simulation.py` with the following command:
```bash
./build/X86/gem5.opt ./configs/run_simulation.py "benchmark_name"
```
where `benchmark_name` is replaced with the name of the benchmark you wish to run. If you desire, you can modify the last section of `run_simulation.py` to add more arguments (like controlling the number of batches without needing to directly modify `sim_config.py`).

The script `dataset_gen/make_all_samples.sh` should perform these steps automatically. At the moment, `make_all_samples.sh` contains a list of benchmarks that it will generate samples for one benchmark at a time. (It may be possible to parallelize this, but I have not tested whether gem5 can handle that; it also will likely require a lot of computing power.)

By default, `run_simulation.py` will output its batches of samples to the `OUTPUT_DIR` path listed in `sim_config.py`. Batches are named `benchname_first-last_?us.pbz2`, where `benchname` is the name of the benchmark, `first-last` is the range of sample numbers contained within, and `?us` is the period length in microseconds (for example, `100us`). Each batch is a bz2-compressed Python pickle, with each pickle containing a list of dictionaries; each dictionary is a sample of the performance counters and other statistics accumulated over a slice of simulated time.

### Computing Bounds
In order to make the final dataset smaller and easier to use, a preprocessing step is used to determine which datapoints are the most useful, along with the smallest and largest observed value for each datapoint. This allows the final dataset to contain only datapoints that differ between samples; it also allows every datapoint to be linearly clamped to [0,1], where 0 represents the smallest observed value for that datapoint and 1 is the largest observed value.

To perform this step, simply run `find_data_bounds.py` while inside the `dataset_gen` directory. By default, this will read every single `.pbz2` batch inside the `raw_samples` directory, group them by sampling period, and output a `dataset_config_?us.ini` file for each same-period group of batches (where `?us` is the period). If you wish to use different locations for the input/output, you can modify the file input/output commands in the script as desired.

The generated `.ini` file contains three sections. The first section lists global settings for dataset creation: currently there is only one setting, which forces the minimum observed value of all datapoints to be 0. Enabling this setting causes the datapoint normalization to match the PerSpectron paper's methods; however, I usually keep this setting disabled, as enabling it causes some datapoints to become practically constant (all scaled values are either above or below the cutoff value of 0.5 used in the PerSpectron paper).

The second section contains the filenames of the batches read. If desired, you can remove batches from this list to exclude them from the final dataset, or add batches to include them. (Note that batches added in this way will not retroactively update the minimum/maximum datapoint values that were found. Similarly, removing batches from this list will not affect the bounds that were already computed. If you want to exclude a batch entirely, move it out of the `raw_samples` directory before running `find_data_bounds.py`.)

The third section contains the minimum and maximum values of all datapoints that were found to be non-constant across all samples (meaning, all the datapoints listed may have their value change in different samples). If desired, you can adjust the min and max values that were computed, or remove the min and max values of a datapoint entirely to exclude the datapoint from the final dataset.

### Building Datasets
Once you have finalized a `dataset_config_?us.ini` file, you can run `generate_dataset.py [config filename]` in `dataset_gen` to create the dataset. The filenames inside the config file will be read, so ensure that all the paths are valid with respect to where you're running the `generate_dataset.py` script. The final dataset will be output as `?us.csv` inside the `datasets` folder by default; again, you can change the output filename and directory as desired by modifying the appropriate line inside `generate_dataset.py`.

The generated csv file will contain a row for every single sample contained in the input batches. Columns include the following:
* `source`: the name of the benchmark used to generate the sample
* `sample_number`: the number of the sample (0 is always the first sample generated from a benchmark)
* `start_time_us` and `end_time_us`: the start and end timestamps, in microseconds since the start of the simulation
* `duration_us`: the sample's length, which should be the same for all samples
* `in/dp_name`: the sample's value for a specific datapoint named `dp_name`; one column per datapoint listed in the config file (the `in/` prefix makes it easy to distinguish datapoint columns from other columns)

## Testing Models
`training/train.py` is a simple program for training and testing attack detection models. The script requires a config file as input; samples are provided in `training/configs/`.

### Config File Format
Each config file has the following sections:
* `File Settings`: contains the location of the dataset to use.
* `Source Settings`: contains a list of each benchmark from the dataset to be used. When listing a benchmark, four additional settings must be provided:
    * Whether the benchmark is benign (i.e. good, false) or malicious (i.e. bad, true)
    * What proportion of the benchmark's samples to use for training (with the rest being used for testing)
    * The probabilistic weight given to the benchmark when randomly picking a sample from all the benchmarks, and when creating aggregate accuracy scores for training
    * The weight given to the benchmark when creating aggreagate accuracy scores for testing
* `Experiment Settings`: contains several settings that control how the experiment is run, including:
    * `samples_per_epoch`: How many samples to train with in each epoch (i.e., how many samples to process before considering the possibility of early stopping).
    * `max_epochs`: The maximum number of epochs to train for.
    * `sample_concat_amount`: If set to 1, samples are used at is. If set to n (where n>1), then new samples are formed by stitching together contigious samples. This provides a quick way to test how well a model performs when given larger sampling periods (without needing to redo the dataset generation). See the description in the sample config files for more info.
    * `malicious_samples_per_epoch`: The number of samples in each epoch that are randomly drawn from batches marked as malicious. This setting can be a comma-separated list to test multiple different values in a single execution of `train.py`.
    * `rng_seed`: the seed used for randomly splitting the data into training and testing sets, and also for randomly drawing samples during training. This can also be a comma-separated list to test multiple different dataset splits.
    * `early_stopping_accuracy`: The minimum training accuracy that must be obtained after an epoch completes in order for training to be immediately stopped.
    * `model`: The name of the model to use. Model names are registered in `training/models.py`.

### Training and Testing
Training starts by parsing the config options, loading the requested dataset, and initializing the model. If the `sample_concat_amount` config option is set to be above 1, then individual samples are concatenated to make larger samples. (For example, if `sample_concat_amount = 2` and there are 5 samples numbered 0 through 4, then four concatenated samples will be created: 0+1, 1+2, 2+3, and 3+4.)

Afterwards, the model is trained for at most `max_epochs` loops, with each loop containing the following steps:
* A random list of False (benign) and True (malicious) values is generated, with the number of Falses equal to the `malicious_samples_per_epoch` config option. This list determines the class from which each training sample is drawn.
* To draw a random sample from a specific class (benign or malicious), a source (or benchmark) is randomly drawn without replacement from a set of all benchmarks. When either training starts or the set becomes empty, the set is repopulated with the name of each source; sources that are assigned a n>1 training weight are replicated n times in the set.
* After a source is selected, a sample is drawn without replacement from the set of all samples made from the source (with the set being refilled if it ever empties). This ensures that all samples from a source are used before any duplicates can occur.
* Once a sample is selected, its data (which is simply an array of values within [0,1], one for each datapoint) is sent to the model, along with whether the sample is benign (False) or malicious (True). The model returns whether or not it guessed correctly, and trains itself based on the sample (if desired).
* Once all samples in an epoch are fed to the model, the model's accuracy for the epoch is compared against `early_stopping_accuracy`. If the accuracy is at or above `early_stopping_accuracy`, then training terminates. Otherwise, the loop restarts.

Once training completes, the model is tested against all training samples again to obtain accurate values for the training sensitivity (accuracy when classifying malicious samples) and specificity (accuracy when classifying benign samples). If sources are given higher training weights, then they will be weighted higher in the sensitivity/specificity calculation.

Additionally, the model will be tested against all of the testing samples that were withheld from training; the aggregate testing sensitivity and specificity will be computed for the model. (Like with the training accuracy, the weights listed in the config file for each source will influence how each source contributes to the aggregate scores.)

Finally, there are multiple ways in which `train.py` will operate, depending on whether or not the `malicious_samples_per_epoch` (MSPE for short) and `rng_seed` config options are given multiple options.
* If only one MSPE value and seed is specified, then a single training session will be run in verbose mode. Verbose mode enables printing out the accuracy after each epoch, printing out the values of the most important model weights after training, and printing out the training/testing accuracies of each source (rather than just the aggregate sensitivity and specificity).
* If there is one MSPE value and multiple seeds, then a simulation will be run with each of the seeds. In addition to printing the testing sensitivity and specificity achieved from each seed, some aggregate stats will also be printed (average sensitivity and specificity, best sensivitity achieved when specificity is perfect, etc.).
* If there are multiple MSPE values and one seed, then a simulation will be run with each of the MSPE values and the same seed (so all simulations will have the same training/testing split). Again, some aggregate stats will also be printed after all the individual results are printed.
* If there are multiple MSPE values and multiple seeds, then a simulation will be run with each possible MSPE-value-and-seed combination. Again, some aggregate stats are printed.


## Models
Currently, the only model in `models.py` is a simple perceptron model, trained in the same way as in [Dynamic Branch Prediction with Perceptrons](https://ieeexplore.ieee.org/document/903263). To briefly summarize the algorithm used:
* The input vector, which is a list of values each within [0,1], is elementwise rounded to the nearest integer. Then, all 0 values are converted to -1.
* The rounded vector is dot-producted with the model's internal weights vector. This produces a score; optionally, a bias value can be added to this score (this feature is currently commented out of the model). If the score is positive, then the model predicts that the input is malicious; otherwise, the input is predicted to be benign.
* If the model mispredicts, then each weight is incremented or decremented so that a correct prediction is more likely if the same input were provided. (Optionally again, the bias value can also be incremented or decremented.)

## Custom Benchmarks
In order to make testing more controlled, the following benchmarks are provided in the `custom_benchmarks` folder so that specific behaviors can be tested with:
* `idletimer.c`: contains a simple program that repeatedly polls the CPU's timestamp counter and increments a counter every millisecond.
* `cacheattack.c`: contains a suite of attacks based on the Prime+Probe, Flush+Reload, and Flush+Flush attacks found in the [Flush+Flush repository](https://github.com/IAIK/flush_flush). The program takes 4 inputs:
    * The attack method: can be either Flush+Reload (`"fr"`), Flush+Flush (`"ff"`), and Prime+Probe (`"pp"`).
    * The attack mode: can be either transmitting (`"tx"`), receiving (`"rx"`), or attack testing (`"test"`).
        * In `tx` mode, the attacking program periodically sends a random bit, either by reloading or evicting a specific cache line. When not attacking, the program idles and spins on the CPU counter.
        * In `rx` mode, the attacking program periodically reads a specific cache line and records the time needed to access it. Again, the program spins when not attacking.
        * In `test` mode, the attacking program periodically randomly sends a random bit by reloading or evicting a cache line, and then reads the bit back by timing a memory access to the cache line. Again, the program spins when not attacking.
    * The attack frequency: can be any integer value in Hertz, or can be `max` (causing the program to always attack and never idle). The datasets included in this repository include 2500Hz, 5000Hz, 7500Hz, and 10000Hz samples.
    * The threshold in CPU cycles used to differentiate between cache hits and misses when in `rx` and `test` modes. (This likely doesn't affect the generated samples, but is included just in case.)
* `randomupdates.c`: like `cacheattack.c`, but instead of idling between attacks, the program randomly reads, updates, and writes to random locations. This program also includes a fourth mode, `benign`, which doesn't attack and only performs the random updates.
All of these benchmarks can be built by running the included Makefile.