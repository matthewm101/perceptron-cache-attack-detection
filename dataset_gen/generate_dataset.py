# This program takes a config file generated using find_data_bounds.py and combines all the samples listed in the config file into a single csv dataset.
# The path of the config file must be passed as the first argument.
# Also, make sure that the list of sample files inside the config file are valid with respect to the current working directory.

# Each datapoint with a provided min and max value is included as a dataset column.
# For each sample, a specific datapoint is linearly scaled to be between 0 and 1, with the min value being assigned 0 and the max being assigned 1.
# The min and max values are auto-generated by find_data_bounds.py for all non-constant datapoints. The config file can be modified to change these.
# Also, there is an option in the config file to completely ignore the config file's minimum values and use 0 as the minimum for all datapoints.

import argparse
import bz2
import configparser
from datetime import datetime
import pickle
import os
import pandas as pd

ap = argparse.ArgumentParser()
ap.add_argument("config", type=argparse.FileType("r"))
args = ap.parse_args()

config = configparser.ConfigParser(allow_no_value=True)
config.optionxform=str
config.read_file(args.config)

force_zero_min = config["Global Settings"]["force_zero_minimum"].lower() in ["true", "yes", "1", "y"]

mins = {}
maxes = {}
for k, v in config["Mins and Maxes"].items():
    if k[-3:] == "min":
        mins[k[:-4]] = float(v)
    elif k[-3:] == "max":
        maxes[k[:-4]] = float(v)
    else:
        print(f"Bad key-value pair in Mins and Maxes section: {k}={v}")
        exit(0)
for m in mins.keys():
    if m not in maxes.keys():
        print(f"Error: datapoint {m} has a minimum but not a maximum")
        exit(0)
for m in maxes.keys():
    if m not in mins.keys():
        print(f"Error: datapoint {m} has a maximum but not a minimum")
        exit(0)

print(f"{len(maxes)} useful arguments and their bounds were found.")

class SampleFile():
    def __init__(self, fname):
        self.fname = fname
        self.full_sample_name = str(os.path.splitext(os.path.split(self.fname)[1])[0])
        splits = self.full_sample_name.split("_")
        self.source_name = splits[0]
        sno_splits = splits[1].split("-")
        self.first_sample_number = int(sno_splits[0])
        self.last_sample_number = int(sno_splits[1])
        self.n_samples = self.last_sample_number + 1 - self.first_sample_number
        self.sample_length_us = int(splits[2][:-2])

filenames = list(config["Files"].keys())
files = [SampleFile(fn) for fn in filenames]

collected_data = {
    "source": [],
    "sample_number": [],
    "start_time_us": [],
    "end_time_us": [],
    "duration_us": [],
}

for k in maxes.keys():
    collected_data[f"in/{k}"] = []

for fileclass in files:
    print(f"Reading {fileclass.fname}...", end="", flush=True)
    with bz2.open(fileclass.fname) as f:
        raw_data = pickle.load(f)
    for i, raw_sample in enumerate(raw_data):
        collected_data["source"].append(fileclass.source_name)
        collected_data["sample_number"].append(fileclass.first_sample_number + i)
        collected_data["start_time_us"].append(int(raw_sample["simulated_begin_time"]) // 1000000)
        collected_data["end_time_us"].append(int(raw_sample["simulated_end_time"]) // 1000000)
        collected_data["duration_us"].append(fileclass.sample_length_us)
        for full_key in maxes.keys():
            v = raw_sample["system"]
            working_key = ""
            for k in full_key.split("."):
                working_key += k
                if working_key in v.keys():
                    v = v[working_key]
                    working_key = ""
                else:
                    working_key += "."
            v = v["value"]
            if force_zero_min:
                v = v / maxes[full_key]
            else:
                v = (v - mins[full_key]) / (maxes[full_key] - mins[full_key])
            collected_data[f"in/{full_key}"].append(v)
    print("Done.")

for k in collected_data.keys():
    assert len(collected_data[k]) == len(collected_data["source"])

print("Creating dataframe...")
df = pd.DataFrame(collected_data)
today = datetime.now().strftime("%Y-%m-%d_%I-%M-%S-%p")

# If you want the final dataset saved somewhere else, change this line.
out_filename = f"../datasets/{files[0].sample_length_us}us_{today}.csv"

print(f"Saving to {out_filename}...")
df.to_csv(out_filename)
print("Done.")