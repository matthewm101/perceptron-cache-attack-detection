# A test that uses all the samples in the 100us_withspec dataset.
# To get averaged statistics, 25 different seeds are tested.
# Note that because sampling is done at 10000Hz but some attacks are done at
# 2500Hz to 7500Hz, it is recommended to set sample_concat_amount to at least 4,
# so that there is at least a possibility of detecting the attack all the time.
# Result: approximately 74% sensitivity and 98% specificity.

[File Settings]
# The path to the dataset.
dataset = ../datasets/100us_withspec.csv

[Source Settings]
# For each dataset you wish to use, add a line with the following format:
# sourcename: class, training_proportion, training_weight, testing_weight

# "sourcename" should be the name of the source. Examples: "astar", "ff-test-10000"

# "class" should be "good", "0", or "false" for benign classes and "bad", "1", or "true" for malicious classes.

# "training_proportion" should be a float between 0 and 1 representing how the class's samples are split.
#   A training proportion of 0 means that all samples are used for TESTING (none for training).
#   A training proportion of 1 means that all samples are used for TRAINING (none for testing).

# "training_weight" is an int representing the proportion that each class will be used in training.
#   For example, if astar has a training weight of 2 and bzip2 has a weight of 1, then astar samples
#   will be using for training twice as often as the bzip2 samples.
#   Also, note that benign and malicious classes will have their proportions handled separately.
#   The ratio of benign to malicious classes used in training/testing is in the [Experiment Settings] section.

# "testing_weight" is the same as training weight, but for sampling during testing phases.

# Example: astar = good, 0.5, 2, 1
idletimer = good, 0.5, 1, 1
ru-benign = good, 0.9, 9, 9

astar = good, 0.9, 1, 1
bzip2 = good, 0.9, 1, 1
gcc = good, 0.9, 1, 1
gobmk = good, 0.9, 1, 1
h264ref = good, 0.9, 1, 1
hmmer = good, 0.9, 1, 1
libquantum = good, 0.9, 1, 1
mcf = good, 0.9, 1, 1
omnetpp = good, 0.9, 1, 1
sjeng = good, 0.9, 1, 1

fr-tx-2500 = bad, 0.5, 1, 1
fr-tx-5000 = bad, 0.5, 1, 1
fr-tx-7500 = bad, 0.5, 1, 1
fr-tx-10000 = bad, 0.5, 1, 1
fr-rx-2500 = bad, 0.5, 1, 1
fr-rx-5000 = bad, 0.5, 1, 1
fr-rx-7500 = bad, 0.5, 1, 1
fr-rx-10000 = bad, 0.5, 1, 1
fr-test-2500 = bad, 0.5, 1, 1
fr-test-5000 = bad, 0.5, 1, 1
fr-test-7500 = bad, 0.5, 1, 1
fr-test-10000 = bad, 0.5, 1, 1
ff-tx-2500 = bad, 0.5, 1, 1
ff-tx-5000 = bad, 0.5, 1, 1
ff-tx-7500 = bad, 0.5, 1, 1
ff-tx-10000 = bad, 0.5, 1, 1
ff-rx-2500 = bad, 0.5, 1, 1
ff-rx-5000 = bad, 0.5, 1, 1
ff-rx-7500 = bad, 0.5, 1, 1
ff-rx-10000 = bad, 0.5, 1, 1
ff-test-2500 = bad, 0.5, 1, 1
ff-test-5000 = bad, 0.5, 1, 1
ff-test-7500 = bad, 0.5, 1, 1
ff-test-10000 = bad, 0.5, 1, 1
pp-tx-2500 = bad, 0.5, 1, 1
pp-tx-5000 = bad, 0.5, 1, 1
pp-tx-7500 = bad, 0.5, 1, 1
pp-tx-10000 = bad, 0.5, 1, 1
pp-rx-2500 = bad, 0.5, 1, 1
pp-rx-5000 = bad, 0.5, 1, 1
pp-rx-7500 = bad, 0.5, 1, 1
pp-rx-10000 = bad, 0.5, 1, 1
pp-test-2500 = bad, 0.5, 1, 1
pp-test-5000 = bad, 0.5, 1, 1
pp-test-7500 = bad, 0.5, 1, 1
pp-test-10000 = bad, 0.5, 1, 1

ru-fr-tx-2500 = bad, 0.5, 1, 1
ru-fr-tx-5000 = bad, 0.5, 1, 1
ru-fr-tx-7500 = bad, 0.5, 1, 1
ru-fr-tx-10000 = bad, 0.5, 1, 1
ru-fr-rx-2500 = bad, 0.5, 1, 1
ru-fr-rx-5000 = bad, 0.5, 1, 1
ru-fr-rx-7500 = bad, 0.5, 1, 1
ru-fr-rx-10000 = bad, 0.5, 1, 1
ru-fr-test-2500 = bad, 0.5, 1, 1
ru-fr-test-5000 = bad, 0.5, 1, 1
ru-fr-test-7500 = bad, 0.5, 1, 1
ru-fr-test-10000 = bad, 0.5, 1, 1
ru-ff-tx-2500 = bad, 0.5, 1, 1
ru-ff-tx-5000 = bad, 0.5, 1, 1
ru-ff-tx-7500 = bad, 0.5, 1, 1
ru-ff-tx-10000 = bad, 0.5, 1, 1
ru-ff-rx-2500 = bad, 0.5, 1, 1
ru-ff-rx-5000 = bad, 0.5, 1, 1
ru-ff-rx-7500 = bad, 0.5, 1, 1
ru-ff-rx-10000 = bad, 0.5, 1, 1
ru-ff-test-2500 = bad, 0.5, 1, 1
ru-ff-test-5000 = bad, 0.5, 1, 1
ru-ff-test-7500 = bad, 0.5, 1, 1
ru-ff-test-10000 = bad, 0.5, 1, 1
ru-pp-tx-2500 = bad, 0.5, 1, 1
ru-pp-tx-5000 = bad, 0.5, 1, 1
ru-pp-tx-7500 = bad, 0.5, 1, 1
ru-pp-tx-10000 = bad, 0.5, 1, 1
ru-pp-rx-2500 = bad, 0.5, 1, 1
ru-pp-rx-5000 = bad, 0.5, 1, 1
ru-pp-rx-7500 = bad, 0.5, 1, 1
ru-pp-rx-10000 = bad, 0.5, 1, 1
ru-pp-test-2500 = bad, 0.5, 1, 1
ru-pp-test-5000 = bad, 0.5, 1, 1
ru-pp-test-7500 = bad, 0.5, 1, 1
ru-pp-test-10000 = bad, 0.5, 1, 1

[Experiment Settings]
# The number of samples to include in each epoch.
samples_per_epoch = 10000

# The maximum number of epochs to train for.
max_epochs = 10

# If this value is set to 1, samples are used as they are, with no modifications.
# If this value is set to n (where n>1), then groups of n timewise contiguous samples are concatenated together to create the samples for the training/testing sets.
# So, if n=2 and there are 5 samples, 4 new samples will be created using the sample sets {0,1}, {1,2}, {2,3}, and {3,4} as input.
# Also, it only makes sense to set this above 1 when using a dataset that has all of its samples ordered such that the end timestamp of sample n is the start timestamp of sample n+1.
# This and sample_minmax_amount cannot both be greater than 1 at the same time.
sample_concat_amount = 4

# The (integer) number of samples in each epoch that should be drawn from the malicious samples.
# This can be a comma separated list of ints, if you want to test out different good/bad splits.
malicious_samples_per_epoch = 500

# The RNG integer seed used for:
#   Splitting the data into training and testing data
#   Randomly drawing samples from the training set
# Replace this with a comma-separated list of ints to test with multiple different splits.
rng_seed = 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25

# The training accuracy that, when reached during an epoch, causes training to end early.
# Lower values may avoid overfitting.
early_stopping_accuracy = 1

# The model to train with. Check models.py for the options.
model = basic_perceptron

[Datapoint Blacklist]
# If you want a datapoint deliberately removed from the training/testing input, add it here.
# You don't need to add an equals sign or anything.
# Example list:
# cpu.iew.memOrderViolationEvents
# cpu.decode.runCycles
# NOTE: you can also list the first part of a datapoint to exclude all datapoints that start with it.
# For example, listing only "cpu" will exclude all cpu-related datapoints.

# I usually remove these datapoints since they involve specific RAM banks and tend to cause overfitting.
mem_ctrls.rdQLenPdf
mem_ctrls.wrQLenPdf
mem_ctrls.dram.perBankRdBursts
mem_ctrls.dram.perBankWrBursts
mem_ctrls.readPktSize
mem_ctrls.writePktSize

[Datapoint Whitelist]
# Same rules as the above list, including the shorthand for capturing groups of datapoints.
# If this list is empty, then it has no effect.
# If a datapoint is captured by both lists, the blacklist has priority.
